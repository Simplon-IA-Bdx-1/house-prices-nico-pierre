{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression on House Prices with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:24:53.689844Z",
     "start_time": "2019-12-03T16:24:53.685575Z"
    }
   },
   "outputs": [],
   "source": [
    "keras_actif = False # Active Keras ou XGBoost\n",
    "if keras_actif:\n",
    "    xgb_actif = False\n",
    "else:\n",
    "    xgb_actif = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:24:55.346603Z",
     "start_time": "2019-12-03T16:24:53.692272Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import transpose\n",
    "from numpy import append\n",
    "from numpy import reshape\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib notebook\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)\n",
    "# tf.random.set_seed(42)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:24:55.630154Z",
     "start_time": "2019-12-03T16:24:55.349477Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape data : (1460, 80)\n",
      "Shape data_num : (1460, 37)\n",
      "Shape data_cat après get_dummies : (1460, 268)\n",
      "\n",
      "Shape data_test : (1459, 79)\n",
      "Shape data_test_num : (1459, 36)\n",
      "Shape data_test_cat après get_dummies : (1459, 256)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('sources/train.csv')\n",
    "data.drop('Id',axis = 1, inplace = True)\n",
    "data.fillna(0, inplace=True)\n",
    "data_num = data.select_dtypes(exclude=['object'])\n",
    "data_cat = data.select_dtypes(include=['object'])\n",
    "\n",
    "data_test = pd.read_csv('sources/test.csv')\n",
    "data_test_id = data_test['Id']\n",
    "data_test.drop('Id',axis = 1, inplace = True)\n",
    "data_test.fillna(0, inplace=True)\n",
    "data_test_num = data_test.select_dtypes(exclude=['object'])\n",
    "data_test_cat = data_test.select_dtypes(include=['object'])\n",
    "\n",
    "# columns where NaN values have meaning e.g. no pool etc.\n",
    "cols_fillna = ['PoolQC','MiscFeature','Alley','Fence','MasVnrType','FireplaceQu',\n",
    "               'GarageQual','GarageCond','GarageFinish','GarageType', 'Electrical',\n",
    "               'KitchenQual', 'SaleType', 'Functional', 'Exterior2nd', 'Exterior1st',\n",
    "               'BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2',\n",
    "               'MSZoning', 'Utilities']\n",
    "\n",
    "# replace 'NaN' with 'None' in these columns\n",
    "for col in cols_fillna:\n",
    "    data[col].fillna('None',inplace=True)\n",
    "    data_test[col].fillna('None',inplace=True)\n",
    "\n",
    "# Convert categorical variable into dummy/indicator variables.\n",
    "data_cat = pd.get_dummies(data_cat)\n",
    "data_test_cat = pd.get_dummies(data_test_cat)\n",
    "\n",
    "print(f\"Shape data : {data.shape}\")\n",
    "print(f\"Shape data_num : {data_num.shape}\")\n",
    "print(f\"Shape data_cat après get_dummies : {data_cat.shape}\")\n",
    "# print('')\n",
    "# print(f\"Features numériques : {list(data_num.columns)}\")\n",
    "# print('')\n",
    "# print(f\"Features catégorielles : {list(data_cat.columns)}\")\n",
    "print('')\n",
    "print(f\"Shape data_test : {data_test.shape}\")\n",
    "print(f\"Shape data_test_num : {data_test_num.shape}\")\n",
    "print(f\"Shape data_test_cat après get_dummies : {data_test_cat.shape}\")\n",
    "# print('')\n",
    "# print(f\"Features numériques : {list(data_test_num.columns)}\")\n",
    "# print('')\n",
    "# print(f\"Features catégorielles : {list(data_test_cat.columns)}\")\n",
    "# print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:24:56.295186Z",
     "start_time": "2019-12-03T16:24:55.632273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Outliers: 15\n",
      "Number of rows without outliers: 1445\n"
     ]
    }
   ],
   "source": [
    "# Suppression des outliers\n",
    "remove_outliers = True\n",
    "if remove_outliers:\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    \n",
    "    anomalies_ratio = 0.01\n",
    "    clf = IsolationForest(contamination = anomalies_ratio,\n",
    "                          behaviour= \" new\",\n",
    "                          random_state = 42)\n",
    "\n",
    "    clf.fit(data_num)\n",
    "    y_noano = clf.predict(data_num)\n",
    "    \n",
    "    y_noano = pd.DataFrame(y_noano, columns = ['Top'])\n",
    "    y_noano[y_noano['Top'] == 1].index.values\n",
    "\n",
    "    data_num = data_num.iloc[y_noano[y_noano['Top'] == 1].index.values]\n",
    "    data_num.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    data_cat = data_cat.iloc[y_noano[y_noano['Top'] == 1].index.values]\n",
    "    data_cat.reset_index(drop = True, inplace = True)\n",
    "    print(\"Number of Outliers:\", y_noano[y_noano['Top'] == -1].shape[0])\n",
    "    print(\"Number of rows without outliers:\", data_num.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['SaleCondition'] == 'Partial'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Num & Cat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:24:56.352871Z",
     "start_time": "2019-12-03T16:24:56.298434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape data : (1445, 305)\n",
      "Shape data_test : (1459, 292)\n"
     ]
    }
   ],
   "source": [
    "# data = pd.concat([data_num, data_cat], ignore_index=False, axis=1)\n",
    "data = pd.concat([data_num, data_cat], ignore_index=False, axis=1)\n",
    "data_test = pd.concat([data_test_num, data_test_cat], ignore_index=False, axis=1)\n",
    "print(f\"Shape data : {data.shape}\")\n",
    "print(f\"Shape data_test : {data_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add missing columns to data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:24:56.430454Z",
     "start_time": "2019-12-03T16:24:56.367661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RoofMatl_Metal', 'Exterior1st_Stone', 'Condition2_RRAe', 'Exterior1st_ImStucc', 'Utilities_NoSeWa', 'SalePrice', 'PoolQC_Fa', 'HouseStyle_2.5Fin', 'Condition2_RRNn', 'Heating_Floor', 'Electrical_0', 'RoofMatl_Membran', 'Exterior2nd_Other', 'MiscFeature_TenC', 'RoofMatl_ClyTile', 'Condition2_RRAn', 'RoofMatl_Roll', 'GarageQual_Ex', 'Electrical_Mix', 'Heating_OthW']\n",
      "\n",
      "Shape data : (1445, 305)\n",
      "Shape data_test : (1459, 305)\n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Reshape colonne similaire\n",
    "missing_col = list(set(data.columns) - set(data_test.columns))\n",
    "print(missing_col)\n",
    "print('')\n",
    "\n",
    "for col in missing_col:\n",
    "    data_test[col] = 0\n",
    "data_test = data_test[data.columns]\n",
    "\n",
    "print(f\"Shape data : {data.shape}\")\n",
    "print(f\"Shape data_test : {data_test.shape}\")\n",
    "print('')\n",
    "missing_col = list(set(data.columns) - set(data_test.columns))\n",
    "print(missing_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract SalePrice from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:24:56.481554Z",
     "start_time": "2019-12-03T16:24:56.434088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train (1445, 304)\n",
      "Y train (1445, 1)\n",
      "X test (1459, 304)\n"
     ]
    }
   ],
   "source": [
    "if 'SalePrice' in data.columns:\n",
    "    Y_train = np.log(data['SalePrice']).to_numpy()\n",
    "    Y_train = Y_train.reshape(Y_train.shape[0],1)\n",
    "    del data['SalePrice']\n",
    "    del data_test['SalePrice']\n",
    "\n",
    "X_train = data.to_numpy()\n",
    "X_test = data_test.to_numpy()\n",
    "\n",
    "print(f\"X train {X_train.shape}\")\n",
    "print(f\"Y train {Y_train.shape}\")\n",
    "print(f\"X test {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scale features\n",
    "\n",
    "\"Normalizing\" the data should help prevent values from \"exploding\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:24:56.548742Z",
     "start_time": "2019-12-03T16:24:56.486204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train (1445, 304)\n",
      "Y train (1445, 1)\n",
      "X test (1459, 304)\n"
     ]
    }
   ],
   "source": [
    "scale_data = True\n",
    "if scale_data:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    scaler_Y = scaler.fit(Y_train)\n",
    "    Y_train = scaler_Y.transform(Y_train)\n",
    "    \n",
    "print(f\"X train {X_train.shape}\")\n",
    "print(f\"Y train {Y_train.shape}\")\n",
    "print(f\"X test {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if keras_actif:\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Input\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    model = Sequential()\n",
    "    model = Sequential([\n",
    "        Input(shape=X_train.shape[1:]),\n",
    "        Dense(28, activation='sigmoid'),\n",
    "        Dense(14, activation='sigmoid'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    print(model.summary())\n",
    "    \n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer=Adam(learning_rate=0.00014, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    )\n",
    "\n",
    "    BATCH_SIZE = 13 #\n",
    "    EPOCHS = 300 # how many iterations over the whole dataset\n",
    "    history = model.fit(X_train, Y_train, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    sns.relplot(x=hist.index, y='loss', kind='line', data=hist)\n",
    "    \n",
    "# BigML\n",
    "# \"network1\": {\n",
    "#     \"learn_residuals\": false,\n",
    "#     \"dropout_rate\": 0,\n",
    "#     \"activation_functions\": [\n",
    "#         \"sigmoid\",\n",
    "#         \"sigmoid\"\n",
    "#     ],\n",
    "#     \"layer_sizes\": [\n",
    "#         28,\n",
    "#         14\n",
    "#     ],\n",
    "#     \"descent_algorithm\": \"adam\",\n",
    "#     \"learning_rate\": 0.00014,\n",
    "#     \"batch_size\": 13,\n",
    "#     \"seed\": \"0-1-2-3-4-5-6-7-8-9-10-cand-1990\",\n",
    "#     \"beta2\": 0.999,\n",
    "#     \"beta1\": 0.9,\n",
    "#     \"epsilon\": 0,\n",
    "#     \"tree_embedding\": false,\n",
    "#     \"max_training_time\": 38964.64948,\n",
    "#     \"batch_normalization\": false,\n",
    "#     \"outputs\": 1\n",
    "# },"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:24:59.082649Z",
     "start_time": "2019-12-03T16:24:56.585200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:05:38] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      " > Results on train :\n",
      " >> MAE :  0.14452049727628635\n",
      "\n",
      " > Results on val :\n",
      " >> MAE :  0.2131217520585995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if xgb_actif:\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    optimisation_gridsearchCV = False\n",
    "    if optimisation_gridsearchCV:\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        gb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "        params = {\n",
    "            'kernel':('linear', 'poly', 'poly', 'rbf', 'rbf'),\n",
    "            'min_child_weight':[4,5],\n",
    "            'gamma':[i/10.0 for i in range(3,6)],\n",
    "            'subsample':[i/10.0 for i in range(6,11)],\n",
    "            'colsample_bytree':[i/10.0 for i in range(6,11)],\n",
    "            'max_depth': [2,3,4]\n",
    "        }\n",
    "\n",
    "        xgb_model = GridSearchCV(xgb_model, params)\n",
    "        xgb_model.fit(X_train, Y_train)\n",
    "\n",
    "        xgb_model.best_estimator_\n",
    "    else:\n",
    "        xgb_model = xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.7, gamma=0.3,\n",
    "             importance_type='gain', kernel='linear', learning_rate=0.1,\n",
    "             max_delta_step=0, max_depth=3, min_child_weight=4, missing=None,\n",
    "             n_estimators=100, n_jobs=1, nthread=None,\n",
    "             objective='reg:squarederror', random_state=0, reg_alpha=0,\n",
    "             reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
    "             subsample=0.8, verbosity=1)\n",
    "        \n",
    "        xgb_model.fit(X_train, Y_train)\n",
    "    \n",
    "    optimisation_flow = True\n",
    "    if optimisation_flow:\n",
    "        from sklearn import metrics\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_split_train,X_split_test, y_split_train, y_split_test = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
    "        xgb_split = xgb.XGBRegressor()\n",
    "        XgBoost = xgb_split.fit(X_split_train, y_split_train)\n",
    "        Y_train_pred_80 = xgb_split.predict(X_split_train) # Prediction sur train (80%)\n",
    "        Y_train_pred_20 = xgb_split.predict(X_split_test) # Prediction sur val (20%)\n",
    "\n",
    "        print(' > Results on train :')\n",
    "#         print(' >> RMSLE : ', np.sqrt(metrics.mean_squared_log_error(y_split_train, Y_train_pred_80)))\n",
    "        print(' >> MAE : ', metrics.mean_absolute_error(y_split_train, Y_train_pred_80))\n",
    "        print()\n",
    "\n",
    "        print (' > Results on val :')\n",
    "#         print(' >> RMSLE : ', np.sqrt(metrics.mean_squared_log_error(y_split_test, Y_train_pred_20)))\n",
    "        print(' >> MAE : ', metrics.mean_absolute_error(y_split_test, Y_train_pred_20))\n",
    "        print()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Kaggle submisison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:24:59.104108Z",
     "start_time": "2019-12-03T16:24:59.084755Z"
    }
   },
   "outputs": [],
   "source": [
    "if keras_actif:\n",
    "    Y_test = model.predict(x=X_test)\n",
    "if xgb_actif:\n",
    "    Y_test = xgb_model.predict(X_test)\n",
    "\n",
    "Y_test = np.exp(scaler_Y.inverse_transform(Y_test))\n",
    "Y_test = np.reshape(Y_test, Y_test.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:25:10.788499Z",
     "start_time": "2019-12-03T16:25:10.697967Z"
    }
   },
   "outputs": [],
   "source": [
    "data_test['Id'] = data_test_id\n",
    "data_test['SalePrice'] = Y_test\n",
    "data_test.drop(data_test.columns.difference(['Id','SalePrice']), 1, inplace=True)\n",
    "data_test.to_csv('storage/allfeatures_.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "ml-workshops"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "livereveal": {
   "scroll": true
  },
  "nteract": {
   "version": "0.15.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.6956481933594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
